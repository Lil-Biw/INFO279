{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desafío 2: Geolocalizar eventos en noticias\n",
    "\n",
    "Utilizando técnicas de tratamiento automático del lenguaje, el objetivo del desafío es estructurar la información de una noticia de la manera siguiente:\n",
    "\n",
    "- event: principal evento descrito en la noticia\n",
    "- category: categoria temática del evento\n",
    "- address: dirección dónde occurió el evento en formato: calle número, comuna, país\n",
    "- latitud: latitud del evento\n",
    "- longitud: longitud del evento\n",
    "\n",
    "Resultado: El formato del archivo CSV output que entregar contiene las columnas siguientes: id_news, event, category, address, latitud, longitud\n",
    "\n",
    "Evaluación del desafío: Se evaluará su método en base a 100 noticias etiquetadas con las métricas siguientes:\n",
    "    • Category: Precision y Recall\n",
    "    • Longitud y Latitud: Exactitud (con una margen de error de 100 metros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primero cargamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopy\n",
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "\n",
    "# Load the required libraries\n",
    "import pandas as pd\n",
    "import googlemaps\n",
    "import gmaps \n",
    "import geopandas as gpd\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "from googlemaps import Client as GoogleMaps\n",
    "from keplergl import KeplerGl\n",
    "from geopy.geocoders import Nominatim\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch\n",
    "from spacy.pipeline.tok2vec import DEFAULT_TOK2VEC_MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "df = pd.read_csv('dataset_agosto2024.csv')\n",
    "nlp_loaded = spacy.load(\"modelo1\")#modelo_clasificador_noticias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "geolocator = Nominatim(user_agent=\"Sophia\")\n",
    "\n",
    "modelAdd=\"mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelAdd)\n",
    "\n",
    "modelAdd = AutoModelForQuestionAnswering.from_pretrained(modelAdd)\n",
    "\n",
    "nlp2 = pipeline(\"question-answering\", model=modelAdd, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('dataset_agosto2024.csv',thousands=',')\n",
    "df2 = df2.dropna()\n",
    "df2 = df2[df2['text'] != '(empty)']\n",
    "\n",
    "questions = [   \"¿En el texto, cual es el evento?\",\n",
    "                \"¿Cual es el pais del evento?\",\n",
    "                \"¿Cual es la comuna del evento?\",\n",
    "                \"¿Cual es la calle del evento?\"\n",
    "                ]\n",
    "\n",
    "df2['event'] = ''\n",
    "df2['category'] = ''\n",
    "df2['address'] = ''\n",
    "df2['latitud'] = ''\n",
    "df2['longitud'] = ''\n",
    "\n",
    "df2 = df2.sample(n=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error geocodificando ubicación: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Per%C3%BA%2C+Per%C3%BA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Creado archivo .csv\n"
     ]
    }
   ],
   "source": [
    "for index, row in df2.iterrows():  \n",
    "    Answer = []\n",
    "    for question in questions: \n",
    "        result = nlp2(question=question, \n",
    "                      tokenizer=tokenizer, \n",
    "                      model=modelAdd, \n",
    "                      context=row['text'])\n",
    "        \n",
    "        Answer.append(result['answer'])\n",
    "    \n",
    "    #Event\n",
    "    df2.at[index,'event'] = Answer[0]\n",
    "    \n",
    "    #Address\n",
    "    lugar = Answer[2] +\", \"+ Answer[1]\n",
    "    df2.at[index,'address'] = lugar\n",
    "    \n",
    "    #Latitude, Longitude\n",
    "    try:\n",
    "        location = geolocator.geocode(lugar)\n",
    "\n",
    "        if location:\n",
    "            df2.at[index,'latitud'] = location.latitude\n",
    "            df2.at[index,'longitud'] = location.longitude\n",
    "\n",
    "        else:\n",
    "            df2.at[index, 'latitud'] = 0.0\n",
    "            df2.at[index, 'longitud'] = 0.0\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Error geocodificando ubicación: {e}\")\n",
    "            df2.at[index, 'latitud'] = 0.0\n",
    "            df2.at[index, 'longitud'] = 0.0      \n",
    "    \n",
    "    #Categoria\n",
    "    doc = nlp_loaded(row['title'])\n",
    "    maxscore = 0.0\n",
    "    maxlabel = ''\n",
    "\n",
    "    for label, score in doc.cats.items():\n",
    "        if (score > maxscore):\n",
    "            maxscore = score\n",
    "            maxlabel = label\n",
    "\n",
    "    df2.at[index,'category'] = maxlabel\n",
    "\n",
    "output_columns = ['id_news', 'event', 'category', 'address', 'latitud', 'longitud']\n",
    "df2[output_columns].to_csv('tarea1-out.csv', index=False)\n",
    "print(\"Creado archivo .csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
