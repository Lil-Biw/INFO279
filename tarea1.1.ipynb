{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Desafío 1 : Construir un modelo de inteligencia artificial capaz de clasificar noticias de prensa según su categoría temática principal.\n",
    "\n",
    "category: sociedad, salud, politica, medioambiente, internacional, entretenimiento, economia, deportes, cultura, cienciatecnologia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primero: cargamos la data que servira para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['entretenimiento', 'tecnologia', 'medio_ambiente', 'ciencia',\n",
       "       'politica', 'internacional', 'accidentes', 'educacion', 'economia',\n",
       "       'salud', 'deportes'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install spacy-lookups-data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Lee el archivo CSV en un DataFrame de pandas\n",
    "\n",
    "df = pd.read_csv(\"train_data.csv\",thousands=',')\n",
    "\n",
    "# Obtén 1000 filas aleatorias del DataFrame\n",
    "random_rows = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# Encuentra las clases únicas en la columna \"clase\"\n",
    "unique_classes = random_rows[\"clase\"].unique()\n",
    "\n",
    "unique_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea la lista de tuplas con las etiquetas de clases\n",
    "data_list = []\n",
    "for _, row in random_rows.iterrows():\n",
    "    title = row[\"title\"]\n",
    "    clase = row[\"clase\"]\n",
    "    cats = {cls: 1 if cls == clase else 0 for cls in unique_classes}\n",
    "    data_list.append((title, {\"cats\": cats}))\n",
    "\n",
    "#data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segundo: Iniciamos entrenamiento con la data anterior\n",
    "\n",
    "### ejecutar 2 veces lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch\n",
    "from spacy.pipeline.tok2vec import DEFAULT_TOK2VEC_MODEL\n",
    "\n",
    "# Datos de entrenamiento\n",
    "TRAIN_DATA = data_list\n",
    "\n",
    "# Inicializar spaCy con el modelo de lenguaje es_core_news_sm\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "train_examples = []\n",
    "\n",
    "for example in TRAIN_DATA:\n",
    "    train_examples.append(Example.from_dict(nlp.make_doc(example[0]), example[1]))\n",
    "\n",
    "def get_examples():\n",
    "    return train_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tercero: definimos el modelo a crear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "            \"@architectures\": \"spacy.TextCatCNN.v2\",\n",
    "            \"exclusive_classes\": True,\n",
    "            \"tok2vec\": DEFAULT_TOK2VEC_MODEL,\n",
    "        }\n",
    "\n",
    "# Crear un componente TextCategorizer con un modelo CNN\n",
    "textcat = nlp.add_pipe(\"textcat\", config={\"model\": model})\n",
    "\n",
    "textcat.initialize(get_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuanto : Comenzamos a entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import compounding\n",
    "\n",
    "# Entrenar el modelo\n",
    "with nlp.select_pipes(enable=\"textcat\"):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for epoch in range(100):\n",
    "        losses = {}\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        # Dividir los datos en lotes y actualizar el modelo\n",
    "        for batch in minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001)):\n",
    "            texts, annotations = zip(*batch)\n",
    "            example = []\n",
    "            # Actualizar el modelo con iteraciones\n",
    "            for i in range(len(texts)):\n",
    "                doc = nlp.make_doc(texts[i])\n",
    "                example.append(Example.from_dict(doc, annotations[i]))\n",
    "            nlp.update(example, drop=0.5, losses=losses)\n",
    "        #print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quinto : Guardamos el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado en disco\n",
    "nlp.to_disk(\"modelo1\")\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "nlp_loaded = spacy.load(\"modelo1\")\n",
    "\n",
    "#modelo_clasificador_noticias\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
